{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset contains 28 by 28 grayscale images of single handwritten digits between 0 and 9. The set consists  70,000 images, the training set having 60,000 and the test set has 10,000."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch as “t”\n",
    "\n",
    "MNSIT dataset from torchvision.datasets\n",
    "\n",
    "torchvision.transforms so we can transform our image to fit our model\n",
    "\n",
    "torch.nn as nn to build our actual neural network\n",
    "\n",
    "matplotlib to visualize our results at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of dataLoader class: because they generate your data on multiple cores in real-time, and straight away feed it into your deep learning model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch_size, which denotes the number of samples contained in each generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = t.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = t.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing to  increase the model efficiency \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple feedforward network that has 3 layers: an input layer, a hidden layer, and an output layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,self.linear1 is the input layer takes 28*28 pixels in each image,100 which is the size of the output.\n",
    "\n",
    "elf.linear2 is the hidden layer, which takes in the output of the previous layer for the input, and has an output size of 50.\n",
    "\n",
    "self.final is the output layer which takes in the output of the previous layer for the input and will output size of 10 since we have 10 values within this dataset (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activation function:output the input if it is positive, otherwise outputting the value of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 100) \n",
    "        self.linear2 = nn.Linear(100, 50) \n",
    "        self.final = nn.Linear(50, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, img): #convert + flatten\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "net = Net()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have the forward function forward() which will actually feed the data through our network.\n",
    "\n",
    "we must use img.view(-1, 28*28) to reshape the images for the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOSS FUNCTION:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cross-entropy for our loss function which is a difference between two probability distributions when given a random set of dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam optimization is a stochastic gradient descent method "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch 10 : when an ENTIRE dataset is passed forward and backward through the neural network ten times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterate over the range of epochs and call the train() function on our model.\n",
    "\n",
    "x will represent the batch of features, and y will represent the batch of targets.\n",
    "\n",
    "optimizer.zero_grad() to set the gradients to 0 before each loss calculation.\n",
    "\n",
    "net(x.view(-1, 28*28)) will pass in our reshaped batch.\n",
    "\n",
    "Then we will use our cross-entropy function to calculate and grab the loss value.\n",
    "\n",
    "After we need loss.backward() to apply the loss back through the network’s parameters.\n",
    "\n",
    "Finally, we need optimizer.step() to optimize weights to account for loss and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_el = nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(net.parameters(), lr=0.001) #e-1\n",
    "epoch = 10\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    net.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        x, y = data\n",
    "        optimizer.zero_grad()\n",
    "        output = net(x.view(-1, 28*28))\n",
    "        loss = cross_el(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop will iterate over the test set and measure for correctness but comparing output to target values. Afterwards, we will receive an accuracy summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.966\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "total=0\n",
    "with t.no_grad():\n",
    "    for data in test_loader:\n",
    "        x, y = data\n",
    "        output = net(x.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if t.argmax(i) == y[idx]:\n",
    "                correct +=1\n",
    "            total +=1\n",
    "print(f'accuracy: {round(correct/total, 3)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISULIZATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(x[9].view(28, 28))\n",
    "plt.show()\n",
    "print(t.argmax(net(x[9].view(-1, 784))[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor and image on matplotlib have the same values,so successfully trained this neural network to recognize handwritten digits from the MNIST dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49add4943758e1df3d36f15bd0abdf4ec890cb071f7d2b8f3c09ec7cb2826b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
